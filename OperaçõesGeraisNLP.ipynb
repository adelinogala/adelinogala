{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Objetivo Geral: Explorar progressivamente técnicas e modelos de NLP para pré-processamento, processamento, treinamento e avaliação de texto.**\n",
        "\n",
        "\n",
        "\n",
        "Normalização\n",
        "\n",
        "A normalização de texto é uma etapa essencial no pré-processamento de dados textuais para aplicações em Machine Learning. Ela busca padronizar o conteúdo, eliminando variações como letras maiúsculas/minúsculas e formas no singular/plural que não alteram o significado da informação. Isso garante que o modelo interprete variações equivalentes de maneira uniforme, facilitando o aprendizado e melhorando a performance. Um exemplo comum de normalização é converter todas as palavras para letras minúsculas, tratando “NLP” e “nlp” como a mesma entidade.\n",
        "\n",
        "Normalization\n",
        "\n",
        "Text normalization is an essential step in the preprocessing of textual data for Machine Learning applications. It aims to standardize the content by eliminating variations such as uppercase/lowercase letters and singular/plural forms that do not change the meaning of the information. This ensures that the model interprets equivalent variations uniformly, facilitating learning and improving performance. A common example of normalization is converting all words to lowercase, treating “NLP” and “nlp” as the same entity."
      ],
      "metadata": {
        "id": "q9VRpsCCEM2Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcOclBlqEFLp"
      },
      "outputs": [],
      "source": [
        "text = \"estou estudando NLP\"\n",
        "text = text.lower()\n",
        "#text: \"estou estudando nlp\"\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming\n",
        "\n",
        "O Stemming é uma técnica de normalização que reduz palavras a sua raiz comum, como “estudando” e “estudo” sendo transformadas em “estud”. Apesar de gerar formas não necessariamente legíveis, essa simplificação ajuda a uniformizar o texto e preservar apenas a informação essencial para o modelo.\n",
        "\n",
        "Stemming\n",
        "\n",
        "Stemming is a normalization technique that reduces words to their common root, such as transforming “estudando” and “estudo” into “estud.” Although it often produces forms that are not necessarily readable, this simplification helps to standardize the text and retain only the essential information for the model."
      ],
      "metadata": {
        "id": "HbEWWUqkEf8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "#Seleciona o idioma português para o stemmer\n",
        "snow_stemmer = SnowballStemmer(language='portuguese')\n",
        "#Quebra o text original em uma lista de plavras\n",
        "text = \"Estamos indo para nossas casas hoje\"\n",
        "words = text.split()\n",
        "#Aplica o stemmer em cada palavra\n",
        "stemmed_words = [snow_stemmer.stem(w) for w in words]\n",
        "# Une a listsa de palavras em uma string única\n",
        "stemmed_text = \" \".join(stemmed_words)\n",
        "#stemmed_text: \"estam indo par noss cas hoj\"\n",
        "print(stemmed_text)"
      ],
      "metadata": {
        "id": "fD9D4egWFESA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords\n",
        "\n",
        "Stopwords são palavras muito frequentes e pouco informativas, como artigos e preposições, que geralmente são removidas do texto para evitar ruído nos modelos. No entanto, é preciso atenção, pois palavras como “não” também costumam estar nessa lista e podem carregar significado importante. Nesse caso, é recomendável ajustar a lista de stopwords para manter termos relevantes.\n",
        "\n",
        "Stopwords\n",
        "\n",
        "Stopwords are very frequent and low-information words, such as articles and prepositions, that are usually removed from text to reduce noise in models. However, caution is needed, as words like “not” are often included in these lists but can carry important meaning. In such cases, it is advisable to adjust the stopword list to retain relevant terms."
      ],
      "metadata": {
        "id": "KLDWztC1Ff_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "text = \"Estudo NLP há 3 dias sim não\"\n",
        "\n",
        "stop_words = set(stopwords.words(\"portuguese\"))\n",
        "\n",
        "# Words to preserve (even if they are in stopwords)\n",
        "preserve_words = {\"sim\", \"não\"}\n",
        "\n",
        "# Update stopwords to exclude preserve_words\n",
        "custom_stopwords = stop_words - preserve_words\n",
        "\n",
        "words = text.split()\n",
        "text_without_stopwords_list = [w for w in words if w not in custom_stopwords]\n",
        "text_without_stopwords = \" \".join(text_without_stopwords_list)\n",
        "\n",
        "print(text_without_stopwords)\n"
      ],
      "metadata": {
        "id": "rI5ThgkkFg0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remoção de Pontuação\n",
        "\n",
        "Pode-se eliminar também sinais de pontuação, como “!”, “?” ou “.”, que em muitas aplicações não são informações relevantes.\n",
        "\n",
        "Punctuation Removal\n",
        "\n",
        "Punctuation marks such as “!”, “?” or “.” can also be removed, as in many applications they do not convey relevant information."
      ],
      "metadata": {
        "id": "p-IJo4gLGVeE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"Estou indo para casa hoje!\"\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "text_without_punct = text.translate(translator)\n",
        "# text_without_punct: “Estou indo para casa hoje\"\n",
        "\n",
        "print(text_without_punct)"
      ],
      "metadata": {
        "id": "Er7zCtpWGcCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Números e Caracteres Especiais\n",
        "\n",
        "Números e caracteres especiais geralmente são removidos ou substituídos por tags (como <NUM>) durante a normalização, especialmente quando não agregam valor à análise. No entanto, essa decisão depende do contexto da aplicação: se os números forem relevantes, devem ser mantidos. Nem todos os modelos lidam bem com tags personalizadas, por isso a escolha do tratamento deve ser feita com critério.\n",
        "\n",
        "Numbers and Special Characters\n",
        "\n",
        "Numbers and special characters are often removed or replaced with tags (such as <NUM>) during normalization, especially when they do not add value to the analysis. However, this decision depends on the application context: if numbers are relevant, they should be retained. Not all models handle custom tags well, so the choice of treatment must be made carefully."
      ],
      "metadata": {
        "id": "Yf-OMuzCG8VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"Estudo NLP há 3 dias\"\n",
        "text_num_tag = re.sub(r\"\\d+\", \"<NUM>\", text)\n",
        "# text_num_tag: Estudo NLP há <NUM> dias\"\n",
        "print(text_num_tag)"
      ],
      "metadata": {
        "id": "yOcVjDh4HEzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS Tagging (Part-of-Speech Tagging)\n",
        "\n",
        "POS tagging (Part-of-Speech tagging) é o processo de identificar e rotular automaticamente as classes gramaticais das palavras em um texto, como substantivos, verbos, adjetivos, etc. Essa etapa é essencial para análises linguísticas mais precisas, permitindo compreender a função de cada palavra na frase e facilitando tarefas como análise sintática, desambiguação de sentidos e extração de informações.\n",
        "\n",
        "POS Tagging (Part-of-Speech Tagging)\n",
        "\n",
        "POS tagging is the process of automatically identifying and labeling the grammatical classes of words in a text, such as nouns, verbs, adjectives, etc. This step is essential for more precise linguistic analysis, as it helps to understand the function of each word in a sentence and supports tasks like syntactic analysis, word sense disambiguation, and information extraction."
      ],
      "metadata": {
        "id": "Fiwpl_z2HeWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Carrega o modelo após o download\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "text = \"Estudo NLP há 3 dias\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"{token.text} -> {token.pos_}\")"
      ],
      "metadata": {
        "id": "VKlnAyWiIWuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency Parsing\n",
        "\n",
        "Dependency parsing é a análise das dependências sintáticas entre as palavras de uma frase, identificando qual palavra é a principal (head) e como as outras se relacionam a ela. O resultado é uma estrutura em forma de árvore que revela a hierarquia e as funções gramaticais no enunciado, sendo fundamental para entender a estrutura e o significado da sentença em tarefas como tradução automática, extração de relações e compreensão de linguagem natural.\n",
        "\n",
        "Dependency Parsing\n",
        "\n",
        "Dependency parsing is the analysis of the syntactic dependencies between the words in a sentence, identifying which word is the main one (head) and how the others relate to it. The result is a tree-structured representation that reveals the hierarchy and grammatical functions within the statement. This is fundamental for understanding the structure and meaning of a sentence in tasks such as machine translation, relation extraction, and natural language understanding."
      ],
      "metadata": {
        "id": "WGBCsmR4I3vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Carrega o modelo de português\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "text = \"Estudo NLP há 3 dias\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Exibe informações de dependência para cada token\n",
        "for token in doc:\n",
        "    print(f\"{token.text:10} | POS: {token.pos_:6} | DEP: {token.dep_:10} | HEAD: {token.head.text}\")"
      ],
      "metadata": {
        "id": "HEFduhi8I-6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition (NER)\n",
        "\n",
        "Named Entity Recognition (NER) é a tarefa de identificar e classificar automaticamente entidades mencionadas em um texto, como nomes de pessoas, organizações, locais, datas e valores. Por exemplo, em “Microsoft é uma empresa de tecnologia fundada em Albuquerque”, o modelo reconhece “Microsoft” como organização e “Albuquerque” como local. NER é amplamente suportado por bibliotecas como SpaCy e serviços cloud como o Amazon Comprehend, inclusive para o português.\n",
        "\n",
        "Named Entity Recognition (NER)\n",
        "\n",
        "Named Entity Recognition (NER) is the task of automatically identifying and classifying entities mentioned in a text, such as names of people, organizations, locations, dates, and values. For example, in “Microsoft is a technology company founded in Albuquerque,” the model recognizes “Microsoft” as an organization and “Albuquerque” as a location. NER is widely supported by libraries such as SpaCy and cloud services like Amazon Comprehend, including for Portuguese."
      ],
      "metadata": {
        "id": "io1Ie_OOJ2gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pt_core_news_sm\n",
        "nlp = pt_core_news_sm.load()\n",
        "doc = nlp('Microsoft é uma empresa fundada em Albuquerque')\n",
        "print([(X.text, X.label_) for X in doc.ents])\n",
        "# Output:\n",
        "# [('Microsoft', 'ORG'), ('Albuquerque', 'LOC')]"
      ],
      "metadata": {
        "id": "V9ljDpvqJ3zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarization\n",
        "\n",
        "Summarization é a tarefa de gerar automaticamente um texto mais curto que preserve as informações mais relevantes de um conteúdo maior. Modelos de summarization são úteis para sintetizar documentos extensos, facilitando a compreensão rápida. Ferramentas como a HuggingFace oferecem modelos eficientes para esse fim, inclusive em português.\n",
        "\n",
        "Summarization\n",
        "\n",
        "Summarization is the task of automatically generating a shorter text that preserves the most relevant information from a longer piece of content. Summarization models are useful for synthesizing lengthy documents, enabling quick understanding. Tools like HuggingFace provide efficient models for this purpose, including support for Portuguese."
      ],
      "metadata": {
        "id": "B0aLYfZbKGxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Carrega o pipeline de sumarização com modelo padrão (usualmente bart ou t5)\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Novo texto de exemplo (tema: inteligência artificial)\n",
        "text = (\"A inteligência artificial (IA) refere-se à simulação de processos de inteligência humana por máquinas, \"\n",
        "        \"especialmente sistemas computacionais. Esses processos incluem aprendizado, raciocínio e autocorreção. \"\n",
        "        \"Aplicações específicas da IA incluem sistemas especialistas, reconhecimento de fala e visão computacional. \"\n",
        "        \"Nos últimos anos, avanços em redes neurais profundas permitiram melhorias significativas em tarefas como \"\n",
        "        \"tradução automática e geração de linguagem natural. Com isso, a IA tem sido cada vez mais utilizada em \"\n",
        "        \"áreas como medicina, finanças, educação e transporte.\")\n",
        "\n",
        "# Executa a sumarização\n",
        "summary = summarizer(text, max_length=130, min_length=30, do_sample=False)\n",
        "\n",
        "# Exibe o resumo\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "KW_KcU-SKHpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question Answering (QA)\n",
        "\n",
        "Question Answering (QA) é a tarefa em que um modelo compreende um texto e responde perguntas com base nesse conteúdo. É especialmente útil para busca e extração de informações em grandes volumes de dados. Modelos de QA, como os disponíveis no HuggingFace, conseguem localizar respostas precisas diretamente no contexto fornecido.\n",
        "\n",
        "Question Answering (QA)\n",
        "\n",
        "Question Answering (QA) is the task in which a model understands a text and answers questions based on that content. It is especially useful for searching and extracting information from large volumes of data. QA models, such as those available on HuggingFace, are capable of locating precise answers directly within the provided context."
      ],
      "metadata": {
        "id": "XYSJ-x9zKmYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Pipeline com modelo compatível com português\n",
        "question_answerer = pipeline(\"question-answering\", model=\"mrm8488/bert-multi-cased-finetuned-xquadv1\")\n",
        "\n",
        "# Novo contexto\n",
        "context = (\n",
        "    \"A inteligência artificial (IA) é um campo da ciência da computação que busca criar sistemas capazes de realizar tarefas \"\n",
        "    \"que normalmente requerem inteligência humana, como reconhecimento de fala, tomada de decisões, e tradução de idiomas. \"\n",
        "    \"Ela tem sido aplicada em diversos setores, como saúde, educação, transporte e segurança. Com o avanço das redes neurais \"\n",
        "    \"e do aprendizado profundo, a IA passou a desempenhar papéis centrais em tecnologias modernas, incluindo assistentes virtuais \"\n",
        "    \"e veículos autônomos.\"\n",
        ")\n",
        "\n",
        "# Pergunta em português\n",
        "result = question_answerer(question=\"Quais são os papéis centrais de IA?\", context=context)\n",
        "\n",
        "# Exibe apenas a resposta\n",
        "print(result['answer'])\n"
      ],
      "metadata": {
        "id": "lQ6YI13TKumc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tradução Automática\n",
        "\n",
        "Tradução automática é a tarefa de converter textos de um idioma para outro. Modelos modernos são capazes de lidar com diferentes estruturas linguísticas e até traduzir siglas corretamente. Em Python, uma opção prática é usar a API do GoogleTranslator para realizar traduções de forma eficiente e precisa.\n",
        "\n",
        "Machine Translation\n",
        "\n",
        "Machine Translation is the task of converting texts from one language to another. Modern models are capable of handling different linguistic structures and even translating acronyms correctly. In Python, a practical option is to use the GoogleTranslator API to perform translations efficiently and accurately."
      ],
      "metadata": {
        "id": "GLLM5I4LLDBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Instale o pacote\n",
        "!pip install deep-translator\n",
        "\n",
        "# 2. Use o tradutor\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "to_translate = (\n",
        "    \"Artificial Intelligence is transforming the way we interact with technology. \"\n",
        "    \"From virtual assistants that respond to voice commands, to recommendation systems \"\n",
        "    \"that help us choose what to watch or buy, AI is becoming an essential part of our daily lives. \"\n",
        "    \"As algorithms become more sophisticated, they are now being used in fields like healthcare, \"\n",
        "    \"education, finance, and autonomous vehicles. However, this rapid development also raises \"\n",
        "    \"ethical concerns and questions about the future of human labor.\"\n",
        ")\n",
        "\n",
        "translated_text = GoogleTranslator(source='auto', target='pt').translate(to_translate)\n",
        "print(translated_text)\n"
      ],
      "metadata": {
        "id": "C83HsG1ILLsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformando Texto em Feature\n",
        "\n",
        "Modelos de Machine Learning como RandomForest, Logistic Regression, Naive Bayes e SVM (Support Vector Machine), entre outros disponíveis no Scikit-learn, exigem que os dados de entrada estejam em formato numérico. Como textos não estruturados não podem ser diretamente convertidos em float, é necessário aplicar técnicas de vetorização, como Bag of Words, TF-IDF ou Word Embeddings (por exemplo, Word2Vec ou GloVe), para transformar palavras em números. Algumas dessas abordagens preservam o contexto semântico, outras apenas a frequência. A escolha da técnica depende do objetivo da modelagem. O dataset “20 newsgroups” é um exemplo popular usado para treinar e avaliar esses modelos com dados textuais classificados por temas.\n",
        "\n",
        "Transforming Text into Features\n",
        "\n",
        "Machine Learning models such as Random Forest, Logistic Regression, Naive Bayes, and SVM (Support Vector Machine), among others available in Scikit-learn, require input data to be in numerical format. Since unstructured text cannot be directly converted into float values, it is necessary to apply vectorization techniques such as Bag of Words, TF-IDF, or Word Embeddings (e.g., Word2Vec or GloVe) to transform words into numbers. Some of these approaches preserve semantic context, while others focus solely on frequency. The choice of technique depends on the modeling objective. The “20 Newsgroups” dataset is a popular example used to train and evaluate these models with textual data categorized by topic."
      ],
      "metadata": {
        "id": "5ZYmUVMrPAN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Carrega os dados de treino\n",
        "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
        "\n",
        "# Exemplo: visualizando uma amostra\n",
        "print(f\"Total de documentos: {len(twenty_train.data)}\")\n",
        "print(f\"Categorias: {twenty_train.target_names}\")\n",
        "print(\"\\nExemplo de documento:\")\n",
        "print(twenty_train.data[0][:500])  # primeiros 500 caracteres do primeiro documento\n"
      ],
      "metadata": {
        "id": "GTvSWbT0POfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag-of-Words\n",
        "\n",
        "Bag-of-Words (BoW) é a técnica mais simples para transformar texto em features. Ela cria uma tabela onde cada coluna representa uma palavra do vocabulário e cada linha representa um documento, com valores indicando a frequência de cada palavra no texto. Embora não preserve o contexto ou a ordem das palavras, é eficaz em muitas aplicações. No Scikit-learn, essa transformação é feita com a classe CountVectorizer.\n",
        "\n",
        "Bag-of-Words (BoW)\n",
        "Bag-of-Words (BoW) is the simplest technique for transforming text into features. It creates a table where each column represents a word in the vocabulary and each row represents a document, with values indicating the frequency of each word in the text. Although it does not preserve word order or context, it is effective in many applications. In Scikit-learn, this transformation is performed using the CountVectorizer class."
      ],
      "metadata": {
        "id": "an8mwFIHPxPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependências se estiver no Google Colab\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Importações\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Carrega o dataset de treino\n",
        "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
        "\n",
        "# Inicializa o CountVectorizer (Bag of Words)\n",
        "count_vect = CountVectorizer(stop_words='english')  # remove stopwords em inglês\n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
        "\n",
        "# Exibe informações\n",
        "print(\"Formato da matriz de vetores (documentos x palavras):\", X_train_counts.shape)\n",
        "print(\"\\nExemplo de palavras no vocabulário:\")\n",
        "print(count_vect.get_feature_names_out()[:20])  # mostra as 20 primeiras palavras\n",
        "print(\"\\nExemplo de documento:\")\n",
        "print(twenty_train.data[0][:500])  # mostra os primeiros 500 caracteres do 1º documento\n"
      ],
      "metadata": {
        "id": "CP_afQodP8V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF\n",
        "\n",
        "TF-IDF (Term Frequency–Inverse Document Frequency) é uma técnica de vetorização que melhora o Bag-of-Words ao normalizar a frequência das palavras pelo tamanho do texto e ao reduzir o peso de termos muito comuns na base. Isso evita que textos mais longos tenham influência desproporcional e destaca palavras mais relevantes. Embora ainda não capture o contexto, é bastante eficaz para diversas tarefas. No Scikit-learn, a classe TfidfVectorizer realiza essa transformação.\n",
        "\n",
        "TF-IDF (Term Frequency–Inverse Document Frequency)\n",
        "\n",
        "TF-IDF is a vectorization technique that improves upon Bag-of-Words by normalizing word frequency according to text length and reducing the weight of very common terms in the corpus. This prevents longer texts from having disproportionate influence and highlights more relevant words. While it still does not capture context, TF-IDF is highly effective for various tasks. In Scikit-learn, this transformation is performed using the TfidfVectorizer class."
      ],
      "metadata": {
        "id": "VFIw0bTrQdsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instala scikit-learn se estiver no Google Colab\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Importações\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Carrega o dataset de treino\n",
        "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
        "\n",
        "# Inicializa o vetor TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')  # remove stopwords em inglês\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(twenty_train.data)\n",
        "\n",
        "# Exibe informações sobre a matriz\n",
        "print(\"Formato da matriz TF-IDF (documentos x palavras):\", X_train_tfidf.shape)\n",
        "\n",
        "# Exibe palavras do vocabulário\n",
        "print(\"\\nExemplo de termos no vocabulário:\")\n",
        "print(tfidf_vectorizer.get_feature_names_out()[:20])\n",
        "\n",
        "# Exemplo de valores TF-IDF (linha 0, primeiros 10 termos não-nulos)\n",
        "print(\"\\nExemplo de vetores TF-IDF do primeiro documento (valores não-nulos):\")\n",
        "import numpy as np\n",
        "row_0 = X_train_tfidf[0].tocoo()\n",
        "for i, j, v in zip(row_0.row, row_0.col, row_0.data):\n",
        "    print(f\"{tfidf_vectorizer.get_feature_names_out()[j]}: {v:.4f}\")\n",
        "    if i >= 10:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "bPYJ4sEVQqn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers) é um modelo pré-treinado desenvolvido pelo Google que transforma textos em features mantendo o contexto e o significado das palavras nas frases. Baseado na arquitetura Transformer, o BERT é capaz de compreender relações semânticas complexas e é amplamente usado em tarefas avançadas de NLP. Existem várias versões do BERT, e uma das mais comuns é a bert-base-uncased, disponível pela biblioteca HuggingFace. É uma das abordagens mais eficazes para representações contextuais de texto.\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers)\n",
        "\n",
        "BERT is a pre-trained model developed by Google that transforms text into features while preserving the context and meaning of words within sentences. Based on the Transformer architecture, BERT can understand complex semantic relationships and is widely used in advanced NLP tasks. There are several versions of BERT, with one of the most common being bert-base-uncased, available through the HuggingFace library. It is one of the most effective approaches for generating contextual text representations."
      ],
      "metadata": {
        "id": "Yr_OeQFGRIpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instale as bibliotecas necessárias (caso esteja no Google Colab)\n",
        "!pip install -U sentence-transformers scikit-learn\n",
        "\n",
        "# Importações\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Exemplo de corpus em português\n",
        "corpus = [\n",
        "    \"A inteligência artificial está transformando o mundo.\",\n",
        "    \"Sistemas de IA já são usados na medicina, educação e transporte.\",\n",
        "    \"Receitas culinárias brasileiras são ricas em sabores e temperos.\",\n",
        "    \"A tecnologia está cada vez mais presente na vida cotidiana.\"\n",
        "]\n",
        "\n",
        "# Carrega modelo multilíngue para embeddings semânticos\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# Gera embeddings BERT\n",
        "embeddings = model.encode(corpus, show_progress_bar=True)\n",
        "\n",
        "# Calcula matriz de similaridade entre os textos\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "# Mostra a matriz de similaridade\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df_sim = pd.DataFrame(similarity_matrix, columns=[f\"Texto {i}\" for i in range(len(corpus))],\n",
        "                      index=[f\"Texto {i}\" for i in range(len(corpus))])\n",
        "print(df_sim.round(2))\n"
      ],
      "metadata": {
        "id": "mzm52t2ERPla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicando modelos de Machine Learning\n",
        "\n",
        "Após transformar o texto em features (como com TF-IDF), o processo de construção de modelos de Machine Learning segue o mesmo fluxo usado com dados estruturados. É possível, por exemplo, treinar um RandomForestClassifier diretamente com os vetores gerados. Para prever novas entradas, basta aplicar a mesma transformação com o objeto previamente ajustado (transform, e não fit_transform). Essa abordagem é válida para qualquer modelo supervisionado, e técnicas como ajuste de parâmetros e avaliação de métricas também se aplicam normalmente.\n",
        "\n",
        "Applying Machine Learning Models\n",
        "\n",
        "After transforming text into features (such as with TF-IDF), the process of building Machine Learning models follows the same workflow used for structured data. For example, a RandomForestClassifier can be trained directly using the generated vectors. To make predictions on new inputs, the same transformation should be applied using the previously fitted object (using transform, not fit_transform). This approach is valid for any supervised model, and techniques such as parameter tuning and metric evaluation apply as usual.\n"
      ],
      "metadata": {
        "id": "T2mGWTCcSO2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instale bibliotecas necessárias (se estiver no Colab)\n",
        "!pip install -U scikit-learn\n",
        "\n",
        "# Importações\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Carrega o dataset de treino\n",
        "print(\"Carregando dataset...\")\n",
        "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
        "\n",
        "# Exibe as categorias disponíveis\n",
        "print(\"\\nCategorias disponíveis:\")\n",
        "for i, cat in enumerate(twenty_train.target_names):\n",
        "    print(f\"{i:2d} - {cat}\")\n",
        "\n",
        "# Transforma os textos em features TF-IDF\n",
        "print(\"\\nTransformando textos em vetores TF-IDF...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(twenty_train.data)\n",
        "\n",
        "# Treina o classificador\n",
        "print(\"Treinando modelo RandomForest...\")\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train_tfidf, twenty_train.target)\n",
        "print(f\"Número de árvores no modelo: {len(clf.estimators_)}\")\n",
        "\n",
        "# Função para testar entrada do usuário\n",
        "def classificar_texto(texto):\n",
        "    X_novo = tfidf_vectorizer.transform([texto])\n",
        "    predicao = clf.predict(X_novo)[0]\n",
        "    return twenty_train.target_names[predicao]\n",
        "\n",
        "# Loop de teste interativo\n",
        "print(\"\\nDigite um texto (ou pressione Enter para sair):\")\n",
        "while True:\n",
        "    entrada = input(\"\\nTexto: \")\n",
        "    if not entrada.strip():\n",
        "        print(\"Encerrado.\")\n",
        "        break\n",
        "    categoria = classificar_texto(entrada)\n",
        "    print(f\"→ Categoria prevista: {categoria}\")\n"
      ],
      "metadata": {
        "id": "BGcbiLEuS1Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similaridade de Textos\n",
        "\n",
        "A comparação entre textos é essencial em várias aplicações de NLP, como busca, deduplicação, e detecção de semântica semelhante. Essa similaridade pode ser calculada de forma vetorial (com BoW, TF-IDF, BERT) ou diretamente entre strings. A biblioteca FuzzyWuzzy facilita essa tarefa ao implementar métricas clássicas como a distância de Levenshtein, oferecendo métodos que consideram variações de ordem, duplicações e substrings. Além dela, há outras métricas relevantes como Jaccard, Jaro e Jaro-Winkler, que permitem medir o grau de similaridade com maior flexibilidade e precisão do que simples comparações binárias.\n",
        "\n",
        "Text Similarity\n",
        "\n",
        "Text comparison is essential in various NLP applications such as search, deduplication, and semantic similarity detection. This similarity can be computed using vector-based methods (like BoW, TF-IDF, or BERT) or directly through string comparisons. The FuzzyWuzzy library facilitates this task by implementing classic metrics like Levenshtein distance, offering methods that account for order variations, duplications, and substrings. In addition to it, other relevant metrics such as Jaccard, Jaro, and Jaro-Winkler allow for measuring similarity with greater flexibility and precision than simple binary comparisons."
      ],
      "metadata": {
        "id": "M_HLe9QUUO9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instale a biblioteca fuzzywuzzy (e python-Levenshtein para melhor desempenho)\n",
        "!pip install fuzzywuzzy[speedup]\n",
        "\n",
        "# Importação da biblioteca\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Frases de exemplo\n",
        "texto1 = \"Eu gosto de café\"\n",
        "texto2 = \"Eu gosto de café\"\n",
        "texto3 = \"Eu não gosto de café\"\n",
        "texto4 = \"Eu gosto de café e meu irmão também\"\n",
        "texto5 = \"De café eu gosto\"\n",
        "texto6 = \"Eu eu gosto gosto de de café café\"\n",
        "\n",
        "print(\"=== Comparações com FuzzyWuzzy ===\")\n",
        "\n",
        "# Comparação direta (match exato)\n",
        "print(\"\\n1. Ratio (match exato)\")\n",
        "print(f\"fuzz.ratio('{texto1}', '{texto2}') = {fuzz.ratio(texto1, texto2)}\")  # 100\n",
        "\n",
        "print(\"\\n2. Ratio (quase igual, com negação)\")\n",
        "print(f\"fuzz.ratio('{texto1}', '{texto3}') = {fuzz.ratio(texto1, texto3)}\")  # ~89\n",
        "\n",
        "# Comparação considerando substrings\n",
        "print(\"\\n3. Partial Ratio (substring)\")\n",
        "print(f\"fuzz.partial_ratio('{texto1}', '{texto4}') = {fuzz.partial_ratio(texto1, texto4)}\")  # 100\n",
        "\n",
        "# Comparação com ordem diferente das palavras\n",
        "print(\"\\n4. Token Sort Ratio (ordem diferente)\")\n",
        "print(f\"fuzz.token_sort_ratio('{texto1}', '{texto5}') = {fuzz.token_sort_ratio(texto1, texto5)}\")  # 100\n",
        "\n",
        "# Comparação ignorando duplicatas\n",
        "print(\"\\n5. Token Set Ratio (duplicatas)\")\n",
        "print(f\"fuzz.token_set_ratio('{texto1}', '{texto6}') = {fuzz.token_set_ratio(texto1, texto6)}\")  # 100\n"
      ],
      "metadata": {
        "id": "M_jqOvnLUw26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similaridade de Cosseno sem Contexto\n",
        "\n",
        "A Similaridade de Cosseno é uma métrica vetorial amplamente usada em NLP para medir o grau de similaridade entre textos, calculando o cosseno do ângulo entre seus vetores representados em um espaço N-dimensional. Ao aplicar técnicas como TF-IDF para transformar textos em vetores, é possível usar cosine_similarity do Scikit-learn para comparar conteúdos. Embora eficaz, essa abordagem não considera o contexto semântico das palavras, focando apenas na distribuição e frequência dos termos. É comum em sistemas de busca e recomendação baseados em conteúdo.\n",
        "\n",
        "Cosine Similarity Without Context\n",
        "\n",
        "Cosine Similarity is a widely used vector-based metric in NLP for measuring the degree of similarity between texts by calculating the cosine of the angle between their vectors in an N-dimensional space. When techniques like TF-IDF are used to transform texts into vectors, it is possible to apply cosine_similarity from Scikit-learn to compare content. While effective, this approach does not consider the semantic context of words, focusing only on term distribution and frequency. It is commonly used in search engines and content-based recommendation systems.\n"
      ],
      "metadata": {
        "id": "NxMs49pVWf98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar o scikit-learn (se necessário)\n",
        "!pip install scikit-learn\n",
        "\n",
        "# Importações\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Lista de textos em português\n",
        "text_list = [\n",
        "    \"Fui ao shopping\",\n",
        "    \"Hoje fui ao shopping\",\n",
        "    \"Gosto de café\"\n",
        "]\n",
        "\n",
        "# Vetorização com TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "text_tfidf = tfidf_vectorizer.fit_transform(text_list)\n",
        "\n",
        "# Cálculo da similaridade de cosseno entre todos os pares\n",
        "cosine_sim = cosine_similarity(text_tfidf, text_tfidf)\n",
        "\n",
        "# Exibição da matriz de similaridade com rótulos\n",
        "df_sim = pd.DataFrame(cosine_sim, index=text_list, columns=text_list)\n",
        "print(\"Matriz de Similaridade de Cosseno:\")\n",
        "print(df_sim.round(2))\n"
      ],
      "metadata": {
        "id": "LovVQFAOWt_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similaridade de Cosseno com Contexto\n",
        "\n",
        "A Similaridade de Cosseno com contexto utiliza modelos como o BERT (ou variações, como paraphrase-xlm-r-multilingual-v1) para gerar vetores que capturam o significado contextual dos textos. Ao aplicar a métrica de cosseno sobre esses vetores, é possível medir com precisão a semelhança semântica entre frases, mesmo quando expressas com palavras diferentes. Essa abordagem supera métodos tradicionais ao considerar o sentido das palavras no enunciado, sendo ideal para tarefas como detecção de paráfrases, busca semântica e agrupamento de ideias semelhantes.\n",
        "\n",
        "Cosine Similarity with Context\n",
        "\n",
        "Cosine Similarity with context leverages models like BERT (or its variations, such as paraphrase-xlm-r-multilingual-v1) to generate vectors that capture the contextual meaning of texts. By applying the cosine metric to these vectors, it becomes possible to accurately measure semantic similarity between sentences—even when expressed with different words. This approach surpasses traditional methods by considering the meaning of words within their context, making it ideal for tasks such as paraphrase detection, semantic search, and clustering of similar ideas."
      ],
      "metadata": {
        "id": "SecfQFuyW-KL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar bibliotecas necessárias (se estiver no Google Colab)\n",
        "!pip install -U sentence-transformers scikit-learn\n",
        "\n",
        "# Importações\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Carrega o modelo multilíngue com suporte ao português\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1')\n",
        "\n",
        "# Lista de textos em português\n",
        "text_list = [\n",
        "    \"Eu moro em Portugal\",\n",
        "    \"Minha casa fica em território português\",\n",
        "    \"Gosto de viajar de avião\"\n",
        "]\n",
        "\n",
        "# Geração dos embeddings com contexto\n",
        "text_bert = model.encode(text_list, show_progress_bar=True)\n",
        "\n",
        "# Cálculo da similaridade de cosseno entre os textos\n",
        "cosine_sim = cosine_similarity(text_bert, text_bert)\n",
        "\n",
        "# Exibição da matriz de similaridade com rótulos\n",
        "df_sim = pd.DataFrame(cosine_sim, index=text_list, columns=text_list)\n",
        "print(\"Matriz de Similaridade com Contexto (BERT):\")\n",
        "print(df_sim.round(2))\n"
      ],
      "metadata": {
        "id": "34SyLzztXLmb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}